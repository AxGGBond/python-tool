#!/usr/bin/env python3
"""
Wordæ–‡æ¡£æ°‘æ³•å…¸è§£æå™¨
ä¸“é—¨ç”¨äºè§£æ.docxæ ¼å¼çš„æ°‘æ³•å…¸æ–‡ä»¶
"""
import openai
import json
import re
import time
import os
from typing import List, Dict, Any, Optional
from docx import Document
from pathlib import Path

class DocxCivilCodeParser:
    """Wordæ–‡æ¡£æ°‘æ³•å…¸è§£æå™¨"""
    
    def __init__(self, api_key: str = None, model: str = "qwen-plus-latest"):
        """
        åˆå§‹åŒ–è§£æå™¨
        
        Args:
            api_key: OpenAI APIå¯†é’¥ï¼Œå¦‚æœä¸ºNoneåˆ™ä»ç¯å¢ƒå˜é‡è·å–
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°ï¼Œé»˜è®¤ä¸ºqwen-plus-latest
        """
        self.api_key = api_key or "sk-a14dc6cb330d4061a8d4396461f166f1"
        self.model = model
        self.client = openai.OpenAI(
            api_key=self.api_key, 
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
            timeout=60.0  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°60ç§’
        )
        
    def read_docx_file(self, file_path: str) -> str:
        """
        è¯»å–Wordæ–‡æ¡£å†…å®¹
        
        Args:
            file_path: Wordæ–‡æ¡£è·¯å¾„
            
        Returns:
            æ–‡æ¡£å†…å®¹å­—ç¬¦ä¸²
        """
        try:
            doc = Document(file_path)
            full_text = []
            
            for paragraph in doc.paragraphs:
                text = paragraph.text.strip()
                if text:  # åªæ·»åŠ éç©ºæ®µè½
                    full_text.append(text)
            
            return '\n'.join(full_text)
        except FileNotFoundError:
            raise FileNotFoundError(f"æ–‡ä»¶ {file_path} ä¸å­˜åœ¨")
        except Exception as e:
            raise Exception(f"è¯»å–Wordæ–‡æ¡£æ—¶å‡ºé”™: {e}")
    
    def extract_articles_from_docx(self, file_path: str) -> List[Dict[str, str]]:
        """
        ä»Wordæ–‡æ¡£ä¸­æå–æ¡æ¬¾ - æ”¹è¿›ç‰ˆ
        
        Args:
            file_path: Wordæ–‡æ¡£è·¯å¾„
            
        Returns:
            æ¡æ¬¾åˆ—è¡¨ï¼Œæ¯ä¸ªæ¡æ¬¾åŒ…å«æ ‡é¢˜å’Œå†…å®¹
        """
        try:
            doc = Document(file_path)
            articles = []
            current_article = {"title": "", "content": ""}
            
            print(f"å¼€å§‹å¤„ç†Wordæ–‡æ¡£ï¼Œå…± {len(doc.paragraphs)} ä¸ªæ®µè½")
            
            for i, paragraph in enumerate(doc.paragraphs):
                text = paragraph.text.strip()
                if not text:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ¡æ¬¾æ ‡é¢˜ï¼ˆåŒ…å«"ç¬¬"å’Œ"æ¡"ï¼‰
                if re.match(r'ç¬¬[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ\d]+æ¡', text):
                    # å¦‚æœå·²æœ‰å½“å‰æ¡æ¬¾ï¼Œå…ˆä¿å­˜
                    if current_article["title"]:
                        articles.append(current_article.copy())
                        print(f"ä¿å­˜æ¡æ¬¾: {current_article['title'][:20]}... (å†…å®¹é•¿åº¦: {len(current_article['content'])})")
                    
                    # åˆ†ç¦»æ ‡é¢˜å’Œå†…å®¹
                    # æŸ¥æ‰¾"ç¬¬xxxæ¡"åé¢çš„å†…å®¹
                    match = re.match(r'(ç¬¬[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ\d]+æ¡)\s*(.*)', text)
                    if match:
                        title = match.group(1)
                        content = match.group(2).strip()
                        current_article = {"title": title, "content": content}
                        print(f"å¼€å§‹æ–°æ¡æ¬¾: {title}")
                        if content:
                            print(f"  æ ‡é¢˜è¡Œå†…å®¹: {content[:50]}...")
                    else:
                        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ï¼Œè¯´æ˜åªæœ‰æ ‡é¢˜
                        current_article = {"title": text, "content": ""}
                        print(f"å¼€å§‹æ–°æ¡æ¬¾: {text}")
                else:
                    # æ·»åŠ åˆ°å½“å‰æ¡æ¬¾å†…å®¹
                    if current_article["title"]:
                        if current_article["content"]:
                            current_article["content"] += "\n" + text
                        else:
                            current_article["content"] = text
                        
                        # æ¯50ä¸ªæ®µè½æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                        if i % 50 == 0:
                            print(f"  æ®µè½ {i}: æ·»åŠ å†…å®¹ (å½“å‰å†…å®¹é•¿åº¦: {len(current_article['content'])})")
            
            # æ·»åŠ æœ€åä¸€ä¸ªæ¡æ¬¾
            if current_article["title"]:
                articles.append(current_article)
                print(f"ä¿å­˜æœ€åæ¡æ¬¾: {current_article['title'][:20]}... (å†…å®¹é•¿åº¦: {len(current_article['content'])})")
            
            print(f"æå–å®Œæˆï¼Œå…± {len(articles)} æ¡æ³•è§„")
            return articles
            
        except Exception as e:
            raise Exception(f"æå–æ¡æ¬¾æ—¶å‡ºé”™: {e}")
    
    def split_articles_by_regex(self, text: str) -> List[str]:
        """
        ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†å‰²æ¡æ¬¾
        
        Args:
            text: æ–‡æ¡£æ–‡æœ¬
            
        Returns:
            åˆ†å‰²åçš„æ¡æ¬¾åˆ—è¡¨
        """
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ‰¾åˆ°æ‰€æœ‰"ç¬¬xxxæ¡"çš„ä½ç½®ï¼Œç„¶åè¿›è¡Œåˆ†å‰²
        articles = re.split(r'(?=ç¬¬[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ\d]+æ¡)', text)
        # ç§»é™¤å¯èƒ½å­˜åœ¨çš„ç©ºå­—ç¬¦ä¸²
        articles = [art for art in articles if art.strip()]
        return articles
    
    def parse_single_article(self, article_text: str, article_index: int, document_context: str = "") -> Dict[str, Any]:
        """
        è§£æå•ä¸ªæ¡æ¬¾
        
        Args:
            article_text: æ¡æ¬¾æ–‡æœ¬
            article_index: æ¡æ¬¾ç´¢å¼•
            document_context: æ–‡æ¡£ä¸Šä¸‹æ–‡ä¿¡æ¯
            
        Returns:
            è§£æç»“æœå­—å…¸
        """
        system_prompt = """
ä½ æ˜¯ä¸€ä¸ªæ³•å¾‹æ–‡ä¹¦ä¿¡æ¯æŠ½å–åŠ©æ‰‹ã€‚
ä½ çš„ä»»åŠ¡æ˜¯ï¼šæ ¹æ®è¾“å…¥çš„æ³•å¾‹æ–‡ä»¶ç±»å‹ï¼Œæå–å…³é”®ä¿¡æ¯ï¼Œå¹¶è½¬æ¢ä¸ºç»“æ„åŒ– JSONã€‚

è¯·éµå¾ªä»¥ä¸‹è§„åˆ™ï¼š

1. **è¯†åˆ«æ–‡ä»¶ç±»å‹**ï¼š
- å¦‚æœæ˜¯ **æ³•å¾‹ / æ³•è§„ / è§„ç« **ï¼Œé€šå¸¸åŒ…å«â€œç¬¬Xæ¡â€ï¼Œè¯·é€æ¡æŠ½å– â†’ ä½¿ç”¨ã€æ¡æ–‡å‹ JSON æ¨¡æ¿ã€‘ã€‚
- å¦‚æœæ˜¯ **é€šçŸ¥ / æŒ‡å¯¼æ„è§ / éƒ¨é—¨è§£é‡Š**ï¼Œé€šå¸¸æ˜¯æ•´ç¯‡æ–‡ä»¶ â†’ ä½¿ç”¨ã€æ–‡ä»¶å‹ JSON æ¨¡æ¿ã€‘ã€‚
- å¦‚æœæ˜¯ **å¸æ³•è§£é‡Š**ï¼Œå¯èƒ½é€æ¡ï¼Œä¹Ÿå¯èƒ½æ•´ç¯‡ â†’ å¦‚æœæœ‰â€œç¬¬Xæ¡â€åˆ™ç”¨ã€æ¡æ–‡å‹ JSON æ¨¡æ¿ã€‘ï¼Œå¦åˆ™ç”¨ã€æ–‡ä»¶å‹ JSON æ¨¡æ¿ã€‘ã€‚
- å¦‚æœæ˜¯ **åˆ¤ä¾‹ / è£åˆ¤æ–‡ä¹¦**ï¼Œè¯·æŠ½å–å½“äº‹äººã€æ¡ˆç”±ã€æ³•é™¢æ„è§ã€è£åˆ¤ç»“æœ â†’ ä½¿ç”¨ã€æ¡ˆä¾‹å‹ JSON æ¨¡æ¿ã€‘ã€‚

2. **è¾“å‡ºæ ¼å¼**ï¼š
- ä¸¥æ ¼è¾“å‡º JSONï¼ˆæ— å¤šä½™æ–‡å­—ï¼‰ã€‚
- å¦‚æœæœ‰å¤šæ¡ï¼ˆå¦‚å¤šä¸ªæ¡æ–‡ï¼‰ï¼Œè¯·æ”¾åœ¨ JSON æ•°ç»„ä¸­ã€‚
- å¦‚æœæ˜¯å•ç¯‡ï¼ˆå¦‚é€šçŸ¥/æ¡ˆä¾‹ï¼‰ï¼Œå¯ä»¥åªè¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡ã€‚

3. **ç¼ºå¤±å­—æ®µ**è¯·å¡«å†™ `null`ï¼Œå­—æ®µå¿…é¡»é½å…¨ã€‚
4. `keywords` æå–è¯¥æ¡æ–‡ä¸­çš„å…³é”®æ³•å¾‹æœ¯è¯­ã€‚
5. `summary` ç”¨ä¸€å¥è¯æ€»ç»“æ¡æ–‡æ ¸å¿ƒè§„å®šã€‚
6. `related_articles` å¦‚æœæœ‰æåŠå…¶ä»–æ¡æ¬¾ï¼Œå°±åˆ—å‡ºæ¥ï¼Œå¦åˆ™ç©ºæ•°ç»„ã€‚
7. **æ¡æ–‡å†…å®¹**ï¼šæ¯ä¸€æ¡çš„â€œcontentâ€å¿…é¡»åŒ…å«ä»è¯¥â€œç¬¬Xæ¡â€å¼€å§‹ï¼Œç›´åˆ°ä¸‹ä¸€ä¸ªâ€œç¬¬Yæ¡â€ä¹‹å‰çš„æ‰€æœ‰æ–‡å­—ï¼ˆåŒ…æ‹¬æ¡æ¬¾å†…çš„æ®µè½ã€åˆ—ä¸¾ã€å‰æ¬¾/åæ¬¾ï¼‰ï¼Œä¸è¦é—æ¼æˆ–æˆªæ–­ã€‚
8.  *content* å†…å®¹è¦è·ŸåŸæ–‡ä¸€æ ·ï¼Œä¸èƒ½åŠ å…¥è‡ªå·±çš„ç†è§£ï¼Œç›´æ¥å¤åˆ¶åŸæ–‡å†…å®¹å³å¯ã€‚

---

### ã€æ¡æ–‡å‹ JSON æ¨¡æ¿ã€‘
```json
{
"law_name": "",
"article_number": "",
"chapter": "",
"content": "",
"summary": "",
"keywords": [],
"scope": "",
"penalty": null,
"exceptions": null,
"related_articles": [],
"effective_date": "",
"amendment_date": "",
"validity_status": "",
 "document_number": "",
"legal_level": "",
"source_url": "",
"tags": [],
"jurisdiction": ""
}
````

### ã€æ–‡ä»¶å‹ JSON æ¨¡æ¿ã€‘ï¼ˆé€šçŸ¥/è§£é‡Šç±»ï¼‰

```json
{
"law_name": "",
"document_type": "",
"document_number": "",
"issuing_body": "",
"issue_date": "",
"effective_date": "",
"amendment_date": null,
"legal_level": "",
"jurisdiction": "",
"content": "",
"summary": "",
"keywords": [],
"scope": "",
"penalty": null,
"exceptions": null,
"related_documents": [],
"source_url": "",
"tags": []
}
```

### ã€æ¡ˆä¾‹å‹ JSON æ¨¡æ¿ã€‘ï¼ˆåˆ¤ä¾‹/è£åˆ¤æ–‡ä¹¦ï¼‰

```json
{
"case_name": "",
"case_number": "",
"court": "",
"trial_date": "",
"document_type": "",
"legal_level": "è£åˆ¤æ–‡ä¹¦",
"jurisdiction": "",
"parties": {
"plaintiff": "",
"defendant": ""
},
"facts": "",
"claims": "",
"defenses": "",
"court_opinion": "",
"judgment": "",
"related_laws": [],
"summary": ""
}
```

---

### ç¤ºä¾‹è¾“å…¥ï¼ˆæ¡æ–‡å‹ï¼‰
```
ã€Šä¸­åäººæ°‘å…±å’Œå›½ä¼ä¸šæ‰€å¾—ç¨æ³•å®æ–½æ¡ä¾‹ã€‹
æ—¶ æ•ˆ æ€§ï¼šç°è¡Œæœ‰æ•ˆ
ä¸­åäººæ°‘å…±å’Œå›½å›½åŠ¡é™¢ä»¤ ç¬¬512å·

ç¬¬äºŒç«  ç¨åŠ¡ç®¡ç†

ç¬¬å…­åæ¡ã€€é™¤å›½åŠ¡é™¢è´¢æ”¿ã€ç¨åŠ¡ä¸»ç®¡éƒ¨é—¨å¦æœ‰è§„å®šå¤–ï¼Œå›ºå®šèµ„äº§è®¡ç®—æŠ˜æ—§çš„æœ€ä½å¹´é™å¦‚ä¸‹ï¼š
ã€€ã€€ï¼ˆä¸€ï¼‰æˆ¿å±‹ã€å»ºç­‘ç‰©ï¼Œä¸º20å¹´ï¼›
ã€€ã€€ï¼ˆäºŒï¼‰é£æœºã€ç«è½¦ã€è½®èˆ¹ã€æœºå™¨ã€æœºæ¢°å’Œå…¶ä»–ç”Ÿäº§è®¾å¤‡ï¼Œä¸º10å¹´ï¼›
ã€€ã€€ï¼ˆä¸‰ï¼‰ä¸ç”Ÿäº§ç»è¥æ´»åŠ¨æœ‰å…³çš„å™¨å…·ã€å·¥å…·ã€å®¶å…·ç­‰ï¼Œä¸º5å¹´ï¼›
ã€€ã€€ï¼ˆå››ï¼‰é£æœºã€ç«è½¦ã€è½®èˆ¹ä»¥å¤–çš„è¿è¾“å·¥å…·ï¼Œä¸º4å¹´ï¼›
ã€€ã€€ï¼ˆäº”ï¼‰ç”µå­è®¾å¤‡ï¼Œä¸º3å¹´ã€‚

ç¬¬å…­åä¸€æ¡ã€€ä»äº‹å¼€é‡‡çŸ³æ²¹ã€å¤©ç„¶æ°”ç­‰çŸ¿äº§èµ„æºçš„ä¼ä¸šï¼Œåœ¨å¼€å§‹å•†ä¸šæ€§ç”Ÿäº§å‰å‘ç”Ÿçš„è´¹ç”¨å’Œæœ‰å…³å›ºå®šèµ„äº§çš„æŠ˜è€—ã€æŠ˜æ—§æ–¹æ³•ï¼Œç”±å›½åŠ¡é™¢è´¢æ”¿ã€ç¨åŠ¡ä¸»ç®¡éƒ¨é—¨å¦è¡Œè§„å®šã€‚

ç¬¬å…­åäºŒæ¡ã€€ç”Ÿäº§æ€§ç”Ÿç‰©èµ„äº§æŒ‰ç…§ä»¥ä¸‹æ–¹æ³•ç¡®å®šè®¡ç¨åŸºç¡€ï¼š
ã€€ã€€ï¼ˆä¸€ï¼‰å¤–è´­çš„ç”Ÿäº§æ€§ç”Ÿç‰©èµ„äº§ï¼Œä»¥è´­ä¹°ä»·æ¬¾å’Œæ”¯ä»˜çš„ç›¸å…³ç¨è´¹ä¸ºè®¡ç¨åŸºç¡€ï¼›
ã€€ã€€ï¼ˆäºŒï¼‰é€šè¿‡æèµ ã€æŠ•èµ„ã€éè´§å¸æ€§èµ„äº§äº¤æ¢ã€å€ºåŠ¡é‡ç»„ç­‰æ–¹å¼å–å¾—çš„ç”Ÿäº§æ€§ç”Ÿç‰©èµ„äº§ï¼Œä»¥è¯¥èµ„äº§çš„å…¬å…ä»·å€¼å’Œæ”¯ä»˜çš„ç›¸å…³ç¨è´¹ä¸ºè®¡ç¨åŸºç¡€ã€‚
ã€€ã€€å‰æ¬¾æ‰€ç§°ç”Ÿäº§æ€§ç”Ÿç‰©èµ„äº§ï¼Œæ˜¯æŒ‡ä¼ä¸šä¸ºç”Ÿäº§å†œäº§å“ã€æä¾›åŠ³åŠ¡æˆ–è€…å‡ºç§Ÿç­‰è€ŒæŒæœ‰çš„ç”Ÿç‰©èµ„äº§ï¼ŒåŒ…æ‹¬ç»æµæ—ã€è–ªç‚­æ—ã€äº§ç•œå’Œå½¹ç•œç­‰ã€‚

```

### ç¤ºä¾‹è¾“å‡º ï¼ˆæ¡æ–‡å‹ï¼‰

```json
[
  {
    "law_name": "ä¸­åäººæ°‘å…±å’Œå›½ä¼ä¸šæ‰€å¾—ç¨æ³•å®æ–½æ¡ä¾‹",
    "article_number": "ç¬¬å…­åæ¡",
    "chapter": null,
    "content": "é™¤å›½åŠ¡é™¢è´¢æ”¿ã€ç¨åŠ¡ä¸»ç®¡éƒ¨é—¨å¦æœ‰è§„å®šå¤–ï¼Œå›ºå®šèµ„äº§è®¡ç®—æŠ˜æ—§çš„æœ€ä½å¹´é™å¦‚ä¸‹ï¼š\nã€€ã€€ï¼ˆä¸€ï¼‰æˆ¿å±‹ã€å»ºç­‘ç‰©ï¼Œä¸º20å¹´ï¼›\nã€€ã€€ï¼ˆäºŒï¼‰é£æœºã€ç«è½¦ã€è½®èˆ¹ã€æœºå™¨ã€æœºæ¢°å’Œå…¶ä»–ç”Ÿäº§è®¾å¤‡ï¼Œä¸º10å¹´ï¼›\nã€€ã€€ï¼ˆä¸‰ï¼‰ä¸ç”Ÿäº§ç»è¥æ´»åŠ¨æœ‰å…³çš„å™¨å…·ã€å·¥å…·ã€å®¶å…·ç­‰ï¼Œä¸º5å¹´ï¼›\nã€€ã€€ï¼ˆå››ï¼‰é£æœºã€ç«è½¦ã€è½®èˆ¹ä»¥å¤–çš„è¿è¾“å·¥å…·ï¼Œä¸º4å¹´ï¼›\nã€€ã€€ï¼ˆäº”ï¼‰ç”µå­è®¾å¤‡ï¼Œä¸º3å¹´ã€‚",
    "summary": "è§„å®šå›ºå®šèµ„äº§æŠ˜æ—§çš„æœ€ä½å¹´é™ã€‚",
    "keywords": ["å›ºå®šèµ„äº§", "æŠ˜æ—§", "æœ€ä½å¹´é™"],
    "scope": "ä¼ä¸šçº³ç¨äºº",
    "penalty": null,
    "exceptions": null,
    "related_articles": ["ç¬¬å…­åæ¡"],
    "effective_date": null,
    "amendment_date": null,
   "document_number": "ä¸­åäººæ°‘å…±å’Œå›½å›½åŠ¡é™¢ä»¤ ç¬¬512å·",
    "validity_status": "ç°è¡Œæœ‰æ•ˆ",
    "legal_level": "æ³•å¾‹",
    "source_url": "",
    "tags": ["å›ºå®šèµ„äº§", "æŠ˜æ—§"],
    "jurisdiction": "å…¨å›½"
  },
  {
    "law_name": "ä¸­åäººæ°‘å…±å’Œå›½ä¼ä¸šæ‰€å¾—ç¨æ³•å®æ–½æ¡ä¾‹",
    "article_number": "ç¬¬å…­åä¸€æ¡",
    "chapter": null,
    "content": "ä»äº‹å¼€é‡‡çŸ³æ²¹ã€å¤©ç„¶æ°”ç­‰çŸ¿äº§èµ„æºçš„ä¼ä¸šï¼Œåœ¨å¼€å§‹å•†ä¸šæ€§ç”Ÿäº§å‰å‘ç”Ÿçš„è´¹ç”¨å’Œæœ‰å…³å›ºå®šèµ„äº§çš„æŠ˜è€—ã€æŠ˜æ—§æ–¹æ³•ï¼Œç”±å›½åŠ¡é™¢è´¢æ”¿ã€ç¨åŠ¡ä¸»ç®¡éƒ¨é—¨å¦è¡Œè§„å®šã€‚",
    "summary": "æ˜ç¡®å¼€é‡‡çŸ³æ²¹ã€å¤©ç„¶æ°”ä¼ä¸šå›ºå®šèµ„äº§æŠ˜æ—§æ–¹æ³•ç”±å›½åŠ¡é™¢è§„å®šã€‚",
    "keywords": ["çŸ³æ²¹", "å¤©ç„¶æ°”", "çŸ¿äº§èµ„æº", "æŠ˜æ—§"],
    "scope": "å¼€é‡‡çŸ³æ²¹å¤©ç„¶æ°”ä¼ä¸š",
    "penalty": null,
    "exceptions": null,
    "related_articles": ["ç¬¬å…­åä¸€æ¡"],
    "effective_date": null,
    "amendment_date": null,
    "validity_status": "ç°è¡Œæœ‰æ•ˆ",
    "document_number": "ä¸­åäººæ°‘å…±å’Œå›½å›½åŠ¡é™¢ä»¤ ç¬¬512å·",
    "legal_level": "æ³•å¾‹",
    "source_url": "",
    "tags": ["çŸ¿äº§èµ„æº", "æŠ˜æ—§"],
    "jurisdiction": "å…¨å›½"
  }
]
```

"""

        # æ„å»ºä½ çš„ Promptï¼Œæ˜ç¡®æŒ‡ç¤ºè¾“å‡ºæ ¼å¼
        user_prompt = f"""
è¯·å°†ä»¥ä¸‹æ°‘æ³•å…¸æ¡æ¬¾å¤„ç†æˆ JSON æ ¼å¼ã€‚

{document_context}

æ¡æ¬¾æ–‡æœ¬ï¼š
{article_text}

è¯·ç›´æ¥è¾“å‡º JSONï¼Œä¸è¦æœ‰å…¶ä»–ä»»ä½•è§£é‡Šã€‚
"""
        print(document_context)
        
        try:
            # è°ƒç”¨ API
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0,
                response_format={"type": "json_object"}
            )

            # æå–æ¨¡å‹å›å¤
            model_response = response.choices[0].message.content
            print(f"  ğŸ¤– æ¨¡å‹å›å¤: {model_response}")

            # å°è¯•è§£æè¿”å›çš„ JSON
            json_output = json.loads(model_response)
            return json_output

        except json.JSONDecodeError as e:
            print(f"å¤„ç†ç¬¬ {article_index+1} æ¡æ—¶å‡ºé”™ï¼Œè¿”å›çš„ä¸æ˜¯æœ‰æ•ˆ JSON: {model_response}")
            return {"error": f"Failed to parse JSON for article {article_index+1}", "raw_response": model_response}
        except Exception as e:
            print(f"å¤„ç†ç¬¬ {article_index+1} æ¡æ—¶å‘ç”Ÿå…¶ä»–é”™è¯¯: {e}")
            return {"error": f"Other error for article {article_index+1}"}
    
    def parse_docx_civil_code(self, input_file: str, output_file: str = None, delay: float = 1.0, use_structured_extraction: bool = True) -> List[Dict[str, Any]]:
        """
        è§£æWordæ–‡æ¡£æ ¼å¼çš„æ°‘æ³•å…¸
        
        Args:
            input_file: è¾“å…¥Wordæ–‡æ¡£è·¯å¾„
            output_file: è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
            delay: APIè°ƒç”¨é—´éš”ï¼ˆç§’ï¼‰ï¼Œé¿å…é€Ÿç‡é™åˆ¶
            use_structured_extraction: æ˜¯å¦ä½¿ç”¨ç»“æ„åŒ–æå–ï¼ˆæ¨èï¼‰
            
        Returns:
            æ‰€æœ‰è§£æç»“æœçš„åˆ—è¡¨
        """
        if not output_file:
            input_path = Path(input_file)
            output_file = input_path.stem + "_parsed.json"
        
        print(f"å¼€å§‹è¯»å–Wordæ–‡æ¡£: {input_file}")
        
        # æå–æ–‡æ¡£ä¸Šä¸‹æ–‡ä¿¡æ¯
        document_context = self.extract_document_context(input_file)
        
        if use_structured_extraction:
            # ä½¿ç”¨ç»“æ„åŒ–æå–æ–¹æ³•
            print("ä½¿ç”¨ç»“æ„åŒ–æå–æ–¹æ³•...")
            articles = self.extract_articles_from_docx(input_file)
            print(f"å…±æ‰¾åˆ° {len(articles)} æ¡æ³•è§„")
            
            all_results = []
            for i, article in enumerate(articles):
                print(f"æ­£åœ¨å¤„ç†ç¬¬ {i+1} / {len(articles)} æ¡: {article['title']}")
                print(f"  å†…å®¹é•¿åº¦: {len(article['content'])} å­—ç¬¦")
                
                # ç»„åˆæ ‡é¢˜å’Œå†…å®¹
                full_text = f"{article['title']}\n{article['content']}"
                result = self.parse_single_article(full_text, i, document_context)
                all_results.append(result)
                
                # æ·»åŠ å»¶è¿Ÿ
                if delay > 0:
                    time.sleep(delay)
        else:
            # ä½¿ç”¨ä¼ ç»Ÿæ–‡æœ¬åˆ†å‰²æ–¹æ³•
            print("ä½¿ç”¨ä¼ ç»Ÿæ–‡æœ¬åˆ†å‰²æ–¹æ³•...")
            full_text = self.read_docx_file(input_file)
            articles = self.split_articles_by_regex(full_text)
            print(f"å…±æ‰¾åˆ° {len(articles)} æ¡æ³•è§„")
            
            all_results = []
            for i, article_text in enumerate(articles):
                print(f"æ­£åœ¨å¤„ç†ç¬¬ {i+1} / {len(articles)} æ¡...")
                result = self.parse_single_article(article_text, i, document_context)
                all_results.append(result)
                
                # æ·»åŠ å»¶è¿Ÿ
                if delay > 0:
                    time.sleep(delay)
        
        # ä¿å­˜ç»“æœ
        print(f"æ­£åœ¨ä¿å­˜ç»“æœåˆ°: {output_file}")
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(all_results, f, ensure_ascii=False, indent=4)
        
        print(f"å¤„ç†å®Œæˆï¼å…±å¤„ç† {len(all_results)} æ¡æ³•è§„ã€‚ç»“æœå·²ä¿å­˜åˆ° {output_file}")
        return all_results
    
    def preview_docx_content(self, file_path: str, max_articles: int = 5) -> None:
        """
        é¢„è§ˆWordæ–‡æ¡£å†…å®¹
        
        Args:
            file_path: Wordæ–‡æ¡£è·¯å¾„
            max_articles: æœ€å¤§é¢„è§ˆæ¡æ¬¾æ•°
        """
        try:
            print(f"é¢„è§ˆWordæ–‡æ¡£: {file_path}")
            print("=" * 50)
            
            if self.extract_articles_from_docx(file_path):
                articles = self.extract_articles_from_docx(file_path)
                print(f"æ–‡æ¡£åŒ…å« {len(articles)} æ¡æ³•è§„")
                print("\nå‰å‡ æ¡æ³•è§„é¢„è§ˆ:")
                
                for i, article in enumerate(articles[:max_articles]):
                    print(f"\n{i+1}. {article['title']}")
                    print(f"   å†…å®¹: {article['content'][:100]}...")
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç»“æ„åŒ–æ¡æ¬¾ï¼Œæ˜¾ç¤ºåŸå§‹æ–‡æœ¬
                text = self.read_docx_file(file_path)
                print("æœªæ‰¾åˆ°ç»“æ„åŒ–æ¡æ¬¾ï¼Œæ˜¾ç¤ºåŸå§‹æ–‡æœ¬é¢„è§ˆ:")
                print(text[:500] + "..." if len(text) > 500 else text)
                
        except Exception as e:
            print(f"é¢„è§ˆæ–‡æ¡£æ—¶å‡ºé”™: {e}")
    
    def test_specific_article(self, file_path: str, article_number: str = "ç¬¬ä¸‰ç™¾ä¸€åä¸€æ¡") -> None:
        """
        æµ‹è¯•ç‰¹å®šæ¡æ–‡çš„æå–
        
        Args:
            file_path: Wordæ–‡æ¡£è·¯å¾„
            article_number: è¦æµ‹è¯•çš„æ¡æ–‡å·
        """
        try:
            doc = Document(file_path)
            print(f"æµ‹è¯•æ¡æ–‡: {article_number}")
            print("=" * 50)
            
            found_article = False
            current_content = []
            
            for i, paragraph in enumerate(doc.paragraphs):
                text = paragraph.text.strip()
                if not text:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯ç›®æ ‡æ¡æ¬¾
                if text.startswith(article_number):
                    found_article = True
                    current_content = [text]
                    print(f"æ‰¾åˆ°æ¡æ–‡å¼€å§‹ï¼Œæ®µè½ {i}: {text}")
                elif found_article:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸‹ä¸€ä¸ªæ¡æ–‡
                    if re.match(r'ç¬¬[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ\d]+æ¡', text):
                        print(f"æ¡æ–‡ç»“æŸï¼Œæ®µè½ {i}: {text}")
                        break
                    else:
                        current_content.append(text)
                        print(f"  æ®µè½ {i}: {text}")
            
            if found_article:
                print(f"\nå®Œæ•´æ¡æ–‡å†…å®¹:")
                print("-" * 30)
                full_content = "\n".join(current_content)
                print(full_content)
                print(f"\nå†…å®¹é•¿åº¦: {len(full_content)} å­—ç¬¦")
            else:
                print(f"æœªæ‰¾åˆ°æ¡æ–‡: {article_number}")
                
        except Exception as e:
            print(f"æµ‹è¯•æ—¶å‡ºé”™: {e}")
    
    def extract_document_context(self, file_path: str) -> str:
        """
        æå–æ–‡æ¡£çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆæ ‡é¢˜ã€å‘æ–‡æœºå…³ç­‰ï¼‰
        
        Args:
            file_path: Wordæ–‡æ¡£è·¯å¾„
            
        Returns:
            æ–‡æ¡£ä¸Šä¸‹æ–‡ä¿¡æ¯å­—ç¬¦ä¸²
        """
        try:
            doc = Document(file_path)
            context_lines = []
            
            # è¯»å–å‰å‡ ä¸ªæ®µè½ï¼Œå¯»æ‰¾æ–‡æ¡£ä¿¡æ¯
            for i, paragraph in enumerate(doc.paragraphs[:20]):  # åªæ£€æŸ¥å‰20ä¸ªæ®µè½
                text = paragraph.text.strip()
                if not text:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ–‡æ¡£æ ‡é¢˜æˆ–å…ƒä¿¡æ¯
                # ä½¿ç”¨æ›´çµæ´»çš„å…³é”®è¯åŒ¹é…ï¼Œè€ƒè™‘ç©ºæ ¼
                text_without_spaces = text.replace(" ", "")
                keywords = [
                    "ä¸­åäººæ°‘å…±å’Œå›½æ°‘æ³•å…¸", "å‘æ–‡æœºå…³", "å‘å¸ƒæ—¥æœŸ", "ç”Ÿæ•ˆæ—¥æœŸ", 
                    "æ—¶æ•ˆæ€§", "æ–‡å·", "ä¸»å¸­ä»¤", "å…¨å›½äººæ°‘ä»£è¡¨å¤§ä¼š"
                ]
                
                if any(keyword in text or keyword in text_without_spaces for keyword in keywords):
                    context_lines.append(text)
                
                # å¦‚æœé‡åˆ°ç¬¬ä¸€ä¸ªæ¡æ–‡ï¼Œåœæ­¢æ”¶é›†
                if re.match(r'ç¬¬[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ\d]+æ¡', text):
                    break
            
            if context_lines:
                context = "\n".join(context_lines)
                print(f"ğŸ“‹ æå–åˆ°æ–‡æ¡£ä¸Šä¸‹æ–‡ä¿¡æ¯:")
                print(context)
                return context
            else:
                return ""
                
        except Exception as e:
            print(f"æå–æ–‡æ¡£ä¸Šä¸‹æ–‡æ—¶å‡ºé”™: {e}")
            return ""


def main():
    """ä¸»å‡½æ•°ç¤ºä¾‹"""
    # ä½¿ç”¨é…ç½®çš„APIå¯†é’¥
    api_key = "sk-a14dc6cb330d4061a8d4396461f166f1"  # è¯·æ›¿æ¢ä¸ºå®é™…çš„APIå¯†é’¥
    model = "qwen-plus-latest"
    base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
    
    print(f"âœ… ä½¿ç”¨é…ç½®çš„APIå¯†é’¥å’Œæ¨¡å‹: {model}")
    print(f"âœ… ä½¿ç”¨base_url: {base_url}")
    
    # åˆ›å»ºè§£æå™¨å®ä¾‹
    parser = DocxCivilCodeParser(api_key=api_key, model=model)
    
    # è¦è§£æçš„Wordæ–‡æ¡£
    input_file = "1.ä¸­åäººæ°‘å…±å’Œå›½æ°‘æ³•å…¸.docx"
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(input_file):
        print(f"æ–‡ä»¶ {input_file} ä¸å­˜åœ¨")
        print("è¯·ç¡®ä¿æ–‡ä»¶åœ¨å½“å‰ç›®å½•ä¸­")
        return
    
    # æµ‹è¯•ç‰¹å®šæ¡æ–‡
    print("\nğŸ” æµ‹è¯•ç¬¬ä¸‰ç™¾ä¸€åä¸€æ¡çš„æå–...")
    parser.test_specific_article(input_file, "ç¬¬ä¸‰ç™¾ä¸€åä¸€æ¡")
    
    # é¢„è§ˆæ–‡æ¡£å†…å®¹
    parser.preview_docx_content(input_file)
    
    # è¯¢é—®æ˜¯å¦ç»§ç»­è§£æ
    response = input("\næ˜¯å¦ç»§ç»­è§£æï¼Ÿ(y/n): ")
    if response.lower() != 'y':
        print("è§£æå·²å–æ¶ˆ")
        return
    
    # è§£ææ°‘æ³•å…¸æ–‡ä»¶
    try:
        results = parser.parse_docx_civil_code(
            input_file=input_file,
            output_file="æ°‘æ³•å…¸è§£æç»“æœ.json",
            delay=5,# æ¯æ¬¡APIè°ƒç”¨é—´éš”5ç§’
            use_structured_extraction=True  # ä½¿ç”¨ç»“æ„åŒ–æå–
        )
        print(f"æˆåŠŸè§£æ {len(results)} æ¡æ³•è§„")
    except Exception as e:
        print(f"è§£æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")


if __name__ == "__main__":
    main()
